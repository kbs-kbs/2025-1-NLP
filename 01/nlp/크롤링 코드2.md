```
!pip install scrapy pandas

import scrapy
from scrapy.crawler import CrawlerProcess
import pandas as pd
from google.colab import files
from scrapy import signals

class DbpiaSpider(scrapy.Spider):
    name = "dbpia"
    allowed_domains = ["dbpia.co.kr"]
    
    def __init__(self, search='', min_year='', max_year='', *args, **kwargs):
        super(DbpiaSpider, self).__init__(*args, **kwargs)
        self.search = search
        self.min_year = min_year
        self.max_year = max_year
        self.results = []

    def start_requests(self):
        base_url = f"https://www.dbpia.co.kr/search/topSearch?searchOption=all&query={self.search}"
        yield scrapy.Request(url=base_url, callback=self.parse_search)

    # parse_search 및 parse_article 메서드는 이전과 동일

def spider_results(spider):
    return spider.results

# 사용자 입력 받기
search = input("검색할 키워드: ").strip()
min_year = input("추출할 최소 년도를 입력: ").strip()
max_year = input("추출할 최대 년도를 입력: ").strip()

process = CrawlerProcess({
    'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
})

spider = DbpiaSpider(search=search, min_year=min_year, max_year=max_year)
# 스파이더 객체 생성 대신 클래스와 인자를 직접 전달
process.crawl(DbpiaSpider, search=search, min_year=min_year, max_year=max_year)
process.start()

results = spider_results(spider)
print(f"크롤링이 완료되었습니다. 총 {len(results)} 개의 논문 정보를 수집했습니다.")

# 결과를 DataFrame으로 변환하고 CSV로 저장하는 코드는 이전과 동일
```
