### Beutiful Soup의 한계
- 웹페이지의 자바스크립트로 생성된 파트는 크롤링이 불가능.
- 스크롤을 내려야 나오는 부분이나, 화면이 움직이는 부분, 계속해서 내용이 바뀌는 댓글 창 등

따라서 자바스크립트로 생성된 페이지의 크롤링에는 파이썬 셀레니움이 필요합니다.

그러나 정적 html에 포함된 내용이라면
Beautiful Soup만으로도 정적 HTML에 포함된 링크를 따라 새 페이지로 이동하여 정보를 추출하고 다시 이전 페이지로 돌아갈 수 있습니다13.

이 과정은 다음과 같이 진행됩니다:

현재 페이지에서 Beautiful Soup를 사용하여 링크를 추출합니다.

추출한 링크로 새로운 HTTP 요청을 보냅니다.

새 페이지의 HTML을 Beautiful Soup로 파싱하여 원하는 정보를 추출합니다.

추출한 정보를 저장합니다.

다음 링크로 이동하거나 원래 페이지로 돌아갑니다.

1. 셀레니움 (Selenium):
장점:
실제 브라우저를 실행하므로 UI 요소 조작 및 테스트가 가능합니다.
다양한 브라우저 및 언어 지원으로 유연한 환경을 제공합니다.
크로스 브라우징 테스트, 웹 스크래핑, 웹 페이지 성능 측정 등 다양한 분야에 활용 가능합니다.
웹 애플리케이션 테스트 자동화에 적합합니다.
단점:
실제 브라우저를 실행하기 때문에 속도가 느린 편입니다.
JavaScript 실행 시간이 추가로 소요될 수 있습니다.
웹 컨트롤(클릭)이 필요할 때 requests 방법보다 Selenium을 사용하는 것이 일반적이지만, 요청만 필요한 경우 requests가 더 유리합니다. 
2. Scrapy:
장점:
서버에서 HTML을 직접 받아오고 분석하는 과정이 매우 효율적이며, 비동기적으로 여러 페이지를 동시에 크롤링할 수 있습니다. 
속도가 빠릅니다. 
단점:
JavaScript 렌더링이 필요한 웹 페이지 크롤링에는 적합하지 않습니다. 
웹 컨트롤(클릭)이 필요한 작업에는 Selenium이 더 적합합니다. 
3. Playwright:
장점:
Selenium보다 더 직관적인 인터페이스와 사용 편의성을 제공합니다.
빠른 속도로 테스트를 수행할 수 있습니다.
단점:
Selenium만큼의 높은 인지도를 가지고 있지는 않습니다.
Selenium과 같은 오래된 UI 요소 식별 방법 대신 HTML 태그, 텍스트 기반의 식별 방법을 권장합니다

```python title:dbpia.py
import scrapy
from scrapy_playwright.page import PageMethod

class DbpiaSpider(scrapy.Spider):
    name = 'dbpia'
    allowed_domains = ['dbpia.co.kr']
    node_count = 0
    node_ids = []

    def start_requests(self):
        query = input('검색어를 입력해주세요: ')
        yield scrapy.Request(
            url=f'https://www.dbpia.co.kr/search/topSearch?searchOption=all&query={query}',
            meta={
                "playwright": True,
                "playwright_include_page": True,
                "playwright_page_methods": [
                    PageMethod('wait_for_selector', '#totalCount')
                ],
            },
            callback=self.parse_result_count
        )

    def parse_result_count(self, response):
        node_count = response.css('#totalCount::text').get().replace('items', '').replace(',', '')
        if node_count > 20:
            page.wait_for_selector('#selectWrapper')
            page.query_selector('#selectWrapper').click()
            page.wait_for_selector('#get100PerPage')
            page.query_selector('#get100PerPage').click()
            page.wait_for_selector('#searchResultList > article:nth-child(21)')
        else:
            page.wait_for_selector('#searchResultList > article:nth-child(1)')

        node_ids += response.css('#searchResultList section.thesisAdditionalInfo.thesis__info::attr(data-nodeid)').getall()
        self.log(f'노드들: {node_ids} 노드 개수: {len(node_ids)}')

        for node_id in node_ids:
            if 'NODE' in node_id:
                yield scrapy.Request(
                    url=f'https://www.dbpia.co.kr/journal/articleDetail?nodeId={node_id}',
                    meta={
                        "playwright": True,
                        "playwright_include_page": True,
                        "playwright_page_methods": []
                    },
                    callback=self.parse_detail_page
                )
            else:
                yield scrapy.Request(
                    url=f'https://www.dbpia.co.kr/journal/detail?nodeId={node_id}',
                    meta={
                        "playwright": True,
                        "playwright_include_page": True,
                        "playwright_page_methods": []
                    },
                    callback=self.parse_detail_page
                )


    def parse_detail_page(self, response):
        item = {
            'site': 'dbpia',
            'title': response.css("#thesisTitle::text").get(),
            'issue_year': response.css("section.thesisDetail__journal.projectDetail__journal > ul > li:nth-child(2) > p.projectDetail__advisoir__desc::text").get()
        }
        
        if item['title']:
            self.log(f"제목: {title}")
        else:
            self.log("제목을 찾지 못했습니다.")
        if item['year_month']:
            self.log(f"발행 연월: {year_month}")
        else:
            self.log("발행 연월을 찾지 못했습니다.")

```


1. yield된 값을 얻으려면 `next()` 함수를 사용하거나 반복문을 통해 제너레이터를 순회해야 합니다.
2. yield는 함수와 반복문에서 쓸 수 있는 중간 저장 + return
3. scrapy-playwright는 내부적으로 `start_requests` 메서드를 제너레이터로 취급하여 `next()`를 사용해 호출합니다. 만약 `start_requests` 함수 안에 `yield`가 없다면 다음과 같은 상황이 발생합니다:

1. 함수가 일반 함수처럼 동작합니다. 제너레이터가 아니므로 `next()`로 호출해도 `StopIteration` 예외가 즉시 발생합니다.
2. Scrapy는 크롤링을 시작할 URL을 얻지 못하게 됩니다. `start_requests`는 Scrapy가 크롤링을 시작할 URL을 생성하는 메서드이기 때문입니다.
3. 결과적으로 크롤러가 아무 작업도 수행하지 않고 즉시 종료됩니다.

따라서 `start_requests` 메서드에는 반드시 최소한 하나 이상의 `yield` 문이 있어야 Scrapy가 정상적으로 동작할 수 있습니다. 일반적으로 `yield scrapy.Request(...)` 형태로 크롤링할 URL을 생성합니다.

만약 `start_requests`에 `yield`를 사용하지 않으려면, 대신 `start_urls` 리스트를 정의하여 Scrapy에게 시작 URL을 제공할 수 있습니다. 이 경우 Scrapy는 내부적으로 이 리스트를 사용하여 초기 요청을 생성합니다.

scrapy-playwright는 start_requests 메서드를 제너레이터로 취급하여 next()를 호출합니다. 이 과정에서 StopIteration 예외가 발생할 때까지 next()를 계속 호출합니다.

```
def start_requests(self):
    urls = [
        'http://www.example.com/1',
        'http://www.example.com/2',
        'http://www.example.com/3'
    ]
    for url in urls:
        yield scrapy.Request(url=url, callback=self.parse)
```

이 메서드가 호출될 때:

첫 번째 next() 호출: 첫 번째 URL에 대한 Request 객체를 반환합니다.

두 번째 next() 호출: 두 번째 URL에 대한 Request 객체를 반환합니다.

세 번째 next() 호출: 세 번째 URL에 대한 Request 객체를 반환합니다.

네 번째 next() 호출: StopIteration 예외가 발생하여 제너레이터가 종료됩니다.

따라서 start_requests는 urls 리스트의 모든 요소에 대해 Request 객체를 생성하고, Scrapy는 이를 순차적으로 처리합니다
