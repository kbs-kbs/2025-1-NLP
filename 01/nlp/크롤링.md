1. 프로젝트 과제명 (주제)
2. 팀번호/팀명 /팀원소개
3. 작품 개요/아이디어 요약
4. 목적 및 목표
4.1. 필요성(Why?)
4.2. 서비스 목표(What?)
4.3. 기대효과
5. 시장분석
5.1. 목표 사용자(Target User)
5.2. 사용자 수요  요약(Users’ Needs)
6. 시스템 기능
6.1. 주요 기능 구성도
6.2. 사용자 시나리오 구성안
7. 유사/경쟁서비스 및 차별성
8. 사용 기술 및 개발 도구
9. 기대 효과



1. 포토그래퍼를 위한 사진 보정 프로그램
조리개 값, 광각, 망원 등
단순한 사진 보정이 아니라 세밀한 값(슬라이더)으로 조정
flickr
내가 가진 싼 사진기로 사진을 찍고 고급 카메라로 찍은 것 처럼 보정.
팀원소개X
요약 간단히
기대효과 똑같이 쓰면 됨
표지, 목차 빼고 ppt 10장
기술성이 제일 중요
사업화 가능 다음
시장성 완성도

3. 파일 정리 프로그램
정보가 많으면 정리 욕구
텍스트 문서나, 사이트 바로가기 등 스크랩만 하면 자동으로 개별 문서들의 연관성을 파악해서 파일 구조를 정리해주는 프로그램

4. 그림 따라그리기 프로그램
글씨 따라쓰기 공책처럼
사진을 불러오면 어떻게 획을 그어야 하는지 화면에 표시해줌







### Beutiful Soup의 한계
- 웹페이지의 자바스크립트로 생성된 파트는 크롤링이 불가능.
- 스크롤을 내려야 나오는 부분이나, 화면이 움직이는 부분, 계속해서 내용이 바뀌는 댓글 창 등

따라서 자바스크립트로 생성된 페이지의 크롤링에는 파이썬 셀레니움이 필요합니다.

그러나 정적 html에 포함된 내용이라면
Beautiful Soup만으로도 정적 HTML에 포함된 링크를 따라 새 페이지로 이동하여 정보를 추출하고 다시 이전 페이지로 돌아갈 수 있습니다13.

이 과정은 다음과 같이 진행됩니다:

현재 페이지에서 Beautiful Soup를 사용하여 링크를 추출합니다.

추출한 링크로 새로운 HTTP 요청을 보냅니다.

새 페이지의 HTML을 Beautiful Soup로 파싱하여 원하는 정보를 추출합니다.

추출한 정보를 저장합니다.

다음 링크로 이동하거나 원래 페이지로 돌아갑니다.

1. 셀레니움 (Selenium):
장점:
실제 브라우저를 실행하므로 UI 요소 조작 및 테스트가 가능합니다.
다양한 브라우저 및 언어 지원으로 유연한 환경을 제공합니다.
크로스 브라우징 테스트, 웹 스크래핑, 웹 페이지 성능 측정 등 다양한 분야에 활용 가능합니다.
웹 애플리케이션 테스트 자동화에 적합합니다.
단점:
실제 브라우저를 실행하기 때문에 속도가 느린 편입니다.
JavaScript 실행 시간이 추가로 소요될 수 있습니다.
웹 컨트롤(클릭)이 필요할 때 requests 방법보다 Selenium을 사용하는 것이 일반적이지만, 요청만 필요한 경우 requests가 더 유리합니다. 
2. Scrapy:
장점:
서버에서 HTML을 직접 받아오고 분석하는 과정이 매우 효율적이며, 비동기적으로 여러 페이지를 동시에 크롤링할 수 있습니다. 
속도가 빠릅니다. 
단점:
JavaScript 렌더링이 필요한 웹 페이지 크롤링에는 적합하지 않습니다. 
웹 컨트롤(클릭)이 필요한 작업에는 Selenium이 더 적합합니다. 
3. Playwright:
장점:
Selenium보다 더 직관적인 인터페이스와 사용 편의성을 제공합니다.
빠른 속도로 테스트를 수행할 수 있습니다.
단점:
Selenium만큼의 높은 인지도를 가지고 있지는 않습니다.
Selenium과 같은 오래된 UI 요소 식별 방법 대신 HTML 태그, 텍스트 기반의 식별 방법을 권장합니다

```python title:dbpia.py
import scrapy
from scrapy_playwright.page import PageMethod

class DbpiaSpider(scrapy.Spider):
    name = 'dbpia'
    allowed_domains = ['dbpia.co.kr']
    node_count = 0
    node_ids = []

    def start_requests(self):
        query = input('검색어를 입력해주세요: ')
        yield scrapy.Request(
            url=f'https://www.dbpia.co.kr/search/topSearch?searchOption=all&query={query}',
            meta={
                "playwright": True,
                "playwright_include_page": True,
                "playwright_page_methods": [
                    PageMethod('wait_for_selector', '#totalCount')
                ],
            },
            callback=self.parse_result_page
        )

    def parse_result_page(self, response):
        page = response.meta["playwright_page"]
        self.node_count = int(response.css('#totalCount::text').get().replace('items', '').replace(',', ''))
        if self.node_count > 20:
            page.wait_for_selector('#selectWrapper')
            page.query_selector('#selectWrapper').click()
            page.wait_for_selector('#get100PerPage')
            page.query_selector('#get100PerPage').click()
            page.wait_for_selector('#searchResultList > article:nth-child(21)')
        else:
            page.wait_for_selector('#searchResultList > article:nth-child(1)')

        self.node_ids += response.css('#searchResultList section.thesisAdditionalInfo.thesis__info::attr(data-nodeid)').getall()
        self.log(f'노드들: {node_ids} 노드 개수: {len(node_ids)}')

        for node_id in self.node_ids:
            if 'NODE' in node_id:
                yield scrapy.Request(
                    url=f'https://www.dbpia.co.kr/journal/articleDetail?nodeId={node_id}',
                    meta={
                        "playwright": True,
                        "playwright_include_page": True,
                        "playwright_page_methods": []
                    },
                    callback=self.parse_detail_page
                )
            else:
                yield scrapy.Request(
                    url=f'https://www.dbpia.co.kr/journal/detail?nodeId={node_id}',
                    meta={
                        "playwright": True,
                        "playwright_include_page": True,
                        "playwright_page_methods": []
                    },
                    callback=self.parse_detail_page
                )


    def parse_detail_page(self, response):
        item = {
            'site': 'dbpia',
            'title': response.css("#thesisTitle::text").get(),
            'issue_year': response.css("section.thesisDetail__journal.projectDetail__journal > ul > li:nth-child(2) > p.projectDetail__advisoir__desc::text").get()
        }
        
        if item['title']:
            self.log(f"제목: {item['title']}")
        else:
            self.log("제목을 찾지 못했습니다.")
        if item['issue_year']:
            self.log(f"발행 연도: {item['issue_year']}")
        else:
            self.log("발행 연도를 찾지 못했습니다.")

```


1. yield된 값을 얻으려면 `next()` 함수를 사용하거나 반복문을 통해 제너레이터를 순회해야 합니다.
2. yield는 함수와 반복문에서 쓸 수 있는 중간 저장 + return
3. scrapy-playwright는 내부적으로 `start_requests` 메서드를 제너레이터로 취급하여 `next()`를 사용해 호출합니다. 만약 `start_requests` 함수 안에 `yield`가 없다면 다음과 같은 상황이 발생합니다:

1. 함수가 일반 함수처럼 동작합니다. 제너레이터가 아니므로 `next()`로 호출해도 `StopIteration` 예외가 즉시 발생합니다.
2. Scrapy는 크롤링을 시작할 URL을 얻지 못하게 됩니다. `start_requests`는 Scrapy가 크롤링을 시작할 URL을 생성하는 메서드이기 때문입니다.
3. 결과적으로 크롤러가 아무 작업도 수행하지 않고 즉시 종료됩니다.

따라서 `start_requests` 메서드에는 반드시 최소한 하나 이상의 `yield` 문이 있어야 Scrapy가 정상적으로 동작할 수 있습니다. 일반적으로 `yield scrapy.Request(...)` 형태로 크롤링할 URL을 생성합니다.

만약 `start_requests`에 `yield`를 사용하지 않으려면, 대신 `start_urls` 리스트를 정의하여 Scrapy에게 시작 URL을 제공할 수 있습니다. 이 경우 Scrapy는 내부적으로 이 리스트를 사용하여 초기 요청을 생성합니다.

scrapy-playwright는 start_requests 메서드를 제너레이터로 취급하여 next()를 호출합니다. 이 과정에서 StopIteration 예외가 발생할 때까지 next()를 계속 호출합니다.

```
def start_requests(self):
    urls = [
        'http://www.example.com/1',
        'http://www.example.com/2',
        'http://www.example.com/3'
    ]
    for url in urls:
        yield scrapy.Request(url=url, callback=self.parse)
```

이 메서드가 호출될 때:

첫 번째 next() 호출: 첫 번째 URL에 대한 Request 객체를 반환합니다.

두 번째 next() 호출: 두 번째 URL에 대한 Request 객체를 반환합니다.

세 번째 next() 호출: 세 번째 URL에 대한 Request 객체를 반환합니다.

네 번째 next() 호출: StopIteration 예외가 발생하여 제너레이터가 종료됩니다.

따라서 start_requests는 urls 리스트의 모든 요소에 대해 Request 객체를 생성하고, Scrapy는 이를 순차적으로 처리합니다



네, 정확합니다. await는 비동기 함수 내에서 다음에 오는 비동기 작업이 완료될 때까지 기다리라는 의미입니다. 즉, await가 포함된 비동기 함수 내에서 해당 작업을 동기적으로 처리하도록 합니다.

await의 주요 특징은 다음과 같습니다:

비동기 함수(async def로 정의된 함수) 내에서만 사용할 수 있습니다.

await 키워드 다음에 오는 비동기 작업의 결과를 기다립니다.

해당 작업이 완료될 때까지 함수의 실행을 일시 중지하고, 완료되면 결과를 반환합니다.

작업이 완료되기를 기다리는 동안 프로그램의 다른 부분이 실행될 수 있도록 제어권을 이벤트 루프에 양보합니다.

예를 들어:

python
async def example():
    result = await some_async_function()
    print(result)
이 코드에서 await some_async_function()은 some_async_function()의 실행이 완료되고 결과를 반환할 때까지 기다립니다. 이 과정에서 example() 함수의 실행은 일시 중지되지만, 프로그램의 다른 부분은 계속 실행될 수 있습니다.



아니 근데 그러면 처음부터 함수를 동기 함수인 채로 두면 되는 거잖아. start_request 함수도 async 함수가 아닌데 왜 굳이 함수는 비동기로 만들고 안에 들어있는 건 동기로 만드니?
답변
매우 좋은 지적입니다. 당신의 의문은 비동기 프로그래밍의 핵심을 짚고 있습니다.

비동기 함수의 필요성:

Scrapy와 Playwright를 함께 사용할 때, 특히 Playwright의 메서드들은 대부분 비동기(async)로 설계되어 있습니다.

이러한 비동기 메서드들을 사용하려면 그것들을 포함하는 함수도 비동기여야 합니다.

start_requests가 동기인 이유:

start_requests는 Scrapy의 기본 인터페이스 중 하나로, 동기 함수로 정의됩니다.

이 함수는 초기 요청을 생성하는 역할만 하며, 실제 비동기 작업은 콜백 함수에서 수행됩니다.

parse_result_page가 비동기인 이유:

이 함수에서 Playwright의 비동기 메서드들(wait_for_selector, click 등)을 사용하기 때문입니다.

이러한 메서드들은 await와 함께 사용해야 하므로, 함수 자체도 async로 선언해야 합니다.

비동기와 동기의 조화:

비동기 함수 내에서 await를 사용하면, 해당 작업이 완료될 때까지 기다리지만 다른 작업들은 계속 실행될 수 있습니다.

이는 I/O 바운드 작업(네트워크 요청 등)에서 특히 효율적입니다.

성능 이점:

여러 요청을 동시에 처리할 수 있어 전체적인 크롤링 속도가 향상됩니다.

리소스 사용을 최적화하여 더 효율적인 크롤링이 가능합니다.

결론적으로, 비동기 함수를 사용하고 그 안에서 await로 동기적 실행을 보장하는 것은, 비동기 라이브러리(Playwright)의 기능을 활용하면서도 코드의 실행 순서를 명확히 하기 위한 것입니다. 이는 성능과 제어의 균형을 맞추는 방법입니다.

Playwright에서 "페이지(Page)"는 브라우저 탭(TAB)과 같다.
